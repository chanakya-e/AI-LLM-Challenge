from transformers import PreTrainedTokenizer
from typing import List
import pdfplumber
from io import BytesIO

class DocumentHandler:
    """Handles extraction of text from PDF documents."""
    
    def __init__(self, pdf_file: bytes, tokenizer: PreTrainedTokenizer, max_tokens: int = 512, overlap: int = 50):
        """Initializes the DocumentHandler with a file-like object and a tokenizer."""
        self.pdf_file = pdf_file
        self.tokenizer = tokenizer
        self.max_tokens = max_tokens
        self.overlap = overlap  # Overlap between chunks to maintain context

    def extract_text(self) -> str:
        """Extracts all text from the PDF."""
        with pdfplumber.open(BytesIO(self.pdf_file)) as pdf:
            text = ""
            for page in pdf.pages:
                text += page.extract_text() or ""
        return text

    def chunk_text(self, text: str) -> List[str]:
        """Splits text into chunks and ensures that no chunk exceeds the maximum token length."""
        input_ids = []
    
        # Tokenize the text in a single pass, avoiding large sequences
        # Break into chunks directly during tokenization using max_length and stride
        tokens = self.tokenizer(
            text,
            max_length=self.max_tokens,  # Ensure that no chunk exceeds the max length
            truncation=True,  # Apply truncation
            stride=self.overlap,  # Overlap between chunks
            return_overflowing_tokens=True,  # Capture overflowing tokens for next chunks
            add_special_tokens=True  # Include special tokens to indicate the beginning and end
        )

        chunks = []
    
        for token_chunk in tokens['input_ids']:  # Get all token chunks generated by the tokenizer
            chunk_text = self.tokenizer.decode(token_chunk, skip_special_tokens=True)
            chunks.append(chunk_text)
    
        return chunks


